- 客户端需要根据当前的viewId找到primaryNode，因此在node数量变化的情况下，需要在reply的消息中加入当前节点的状态。
- 原算法中的节点是连续编号的，如果存在节点删除，会导致序号不连续，因此每一个点（包括客户端）需要保存一个livenode[]的数组，以及当前的节点总数N。primaryNode的选择策略变为   p=livenode[ v mode N ]
- sever节点中的livenode状态天然同步，因为节点加入和删除的时候需要pause the world。客户端的同步实在reply中实现的。假设客户端状态失败，会进行重传，重传之后网络内部通过广播会找到正确节点，然后最终通过reply达成同步

节点加入的流程：

- 节点就像客户端一样进行操作==>[单播或者广播的方式向主节点请求加入网络]

- 主节点开始pause the world。然后收到任何的客户端请求都缓存起来。向所有节点发送newNode的消息。至此所有的节点都不在接受客户端请求【所有的节点都返回pause的消息，客户端会延时请求】。

  等到所有节点协商一致只有，改变livenode数组。所有节点恢复正常，primary开始执行缓存起来的消息。

- 如何协商？？？？每个节点都有一个身份验证算法，视具体系统而定。这个算法只是做身份状态同步的。

节点删除的流程

- 每一个节点维护一个计数器数组，记录每一个点的错误次数
- 由主节点发起节点删除的操作，同样是pause-the-world
- 其他节点根据自身的计数器判断是否同意
- 最终协商一致，修改内部livenode数组。

思考：pause the world之后，应该去找到最近的checkpoint，然后主节点重做checkpoint之后的操作